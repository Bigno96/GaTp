# Solve Token Passing with Message-Aware Graph Attention model as in MAGAT
# arXiv:2011.13219v2
cuda: True  # device for loading tensors. If True, cuda is used, if False cpu is used instead
gpu_device: 0   # number of device to use when cuda is enabled
cuda_benchmark: True  # if True, use cuDNN benchmarking to select fastest convolution algorithm
amp: False  # if True, Automatic Mixed Precision cuda acceleration is active

seed: 1337  # seed for torch

# folder paths, only inside project scope
data_root: "GaTp/datasets"
exp_folder: "GaTp/experiments"


# model hyper parameters
# 1) CNN, feature extractor
cnn_model: 'res'  # type of cnn: residual (network)
cnn_in_channels: 3  # number of input channels
cnn_blocks_size: [32, 64, 128]  # number of channels for cnn blocks
cnn_depths: [1, 1, 1]   # number of blocks that compose each layer of the cnn
                        # blocks in the i-th layer all have channels num = cnn_blocks_size[i]
cnn_out_features: 128   # number of 'extracted' features from the input FOV by the CNN (decoder output)
cnn_decoder_expansion: 8  # feature expansion factor in the intermediate layer of res-net decoder
use_down_sampling: True   # if True, down sampling is used in the res net
cnn_dropout: True   # if True, add dropout to res net decoder
cnn_dropout_rate: 0.2   # ratio of dropout, if used

feature_compression: True   # if True, an additional MLP layer is placed at the end of the CNN
feature_compr_out: 32   # output from features compression. These coincide with skip connection features
feature_compr_hidden_feat: []   # number of features of the hidden layers in the MLP
feature_compr_dropout: False   # if True, add dropout to MLP
feature_compr_dropout_rate: 0.2   # ratio of dropout, if used
feature_compr_learn_bias: True  # if True, learn bias in the MLP


# if True, a skip connection is added before and after the GNN
skip_connection: True


# 2) GNN -> GAT with GSO
gnn_model: 'gat_gso'  # type of gnn: graph convolution attentional network
gnn_hidden_features: []   # tuple of int, e.g. (32, 64, 128), vector of INPUT features of hidden layers
                          # should be same size of feature_compr_out if used, else cnn_out_features
                          # len = number of desired graph filtering layers - 1
graph_filter_taps: [2]  # tuple of int, 'power' of the GSO. Vector with number of filter taps for each layer
                        # len = number of graph filtering layers
attention_heads: [4]  # tuple of int, vector with number of attention heads for each layer
                      # len = number of graph filtering layers
attention_concat: True  # if True, concatenate the output of the attention heads. False, average the output

# 3) MLP, map features to actions
mlp_hidden_features: []   # tuple of int, vector of INPUT features of hidden layers
                          # len = number of layers - 1
mlp_dropout: False   # if True, add dropout to MLP
mlp_dropout_rate: 0.2   # ratio of dropout, if used
mlp_learn_bias: True  # if True, learn bias in the MLP


# training parameters
batch_size: 16  # batch size for training
shuffle_train: True   # if True, train dataset is shuffled
valid_batch_size: 1   # batch size for validation (simulation)
test_batch_size: 1  # batch size for testing (simulation)

data_loader_workers: 4  # num of processes for data loading
pin_memory: True  # if True, data loader will copy Tensors into CUDA pinned memory before returning them

learning_rate: 0.0001  # learning rate of optimizer
weight_decay: 0.00001   # weight decay of optimizer (implemented as L2 penalty)
max_epoch: 300  # maximum number of epochs of training

validate_every: 8   # how many epochs between each validation
print_valid_every: 10  # how many timesteps between each validation print update
log_interval: 5000    # how many batches passes between screen updates for training
max_step_factor: 2  # multiplicative factor for simulation max step (target makespan * max_step_factor)

# dataset aggregation
expert_type: 'tp'   # expert to use
max_new_instances: 800  # maximum number of new augmenting instances to find for each epoch
                        # each instance is solved by the expert, generating N training entries
                        # with N bounded by max_new_instances * max expert makespan
selection_percentage: 0.25  # percentage of most conflicting timesteps to take from each simulation
timeout: 10   # seconds, maximum time of expert execution before cutting it off
timestep_limit: 10   # for how many timestep to run the expert execution, 0 means no limit