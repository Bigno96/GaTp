# Solve Token Passing with Message-Aware Graph Attention model as in MAGAT
# arXiv:2011.13219v2
cuda: True  # device for loading tensors. If True, cuda is used, if False cpu is used instead
gpu_device: 0   # number of device to use when cuda is enabled
cuda_benchmark: True  # if True, use cuDNN benchmarking to select fastest convolution algorithm

seed: 1337  # seed for torch

# folder paths, only inside project scope
data_root: "GaTp/datasets"
exp_folder: "GaTp/experiments"


# model hyper parameters
# 1) CNN, feature extractor
cnn_model: 'res'  # type of cnn: residual (network)
cnn_in_channels: 3  # number of input channels
cnn_out_features: 512   # number of 'extracted' features from the input FOV by the CNN
cnn_blocks_size: [32, 64, 128, 256]  # number of channels for cnn blocks
cnn_depths: [3, 3, 3, 3]   # number of blocks that compose each layer of the cnn
                           # blocks in the i-th layer all have channels num = cnn_blocks_size[i]
use_down_sampling: True   # if True, down sampling is used in the res net
feature_compression: True   # if True, an additional MLP layer is placed at the end of the CNN
feature_compr_out: 32   # output from features compression. These coincide with skip connection features
feature_compr_hidden_feat: [128, 64]   # number of features of the hidden layers in the MLP
feature_compr_dropout: True   # if True, add dropout to MLP
feature_compr_dropout_rate: 0.2   # ratio of dropout, if used

# if True, a skip connection is added before and after the GNN
skip_connection: True

# 2) GNN -> GAT with GSO
gnn_model: 'gat_gso'  # type of gnn: graph convolution attentional network
gnn_hidden_features: []   # tuple of int, e.g. (32, 64, 128), vector of INPUT features of hidden layers
                          # should be same size of feature_compr_out if used, else cnn_out_features
                          # len = number of desired graph filtering layers - 1
graph_filter_taps: [2]  # tuple of int, 'power' of the GSO. Vector with number of filter taps for each layer
                        # len = number of graph filtering layers
attention_heads: [4]  # tuple of int, vector with number of attention heads for each layer
                      # len = number of graph filtering layers
attention_concat: True  # if True, concatenate the output of the attention heads. False, average the output

# 3) MLP, map features to actions
mlp_hidden_features: [128, 64]   # tuple of int, vector of INPUT features of hidden layers
                            # len = number of layers - 1
mlp_dropout: True   # if True, add dropout to MLP
mlp_dropout_rate: 0.3   # ratio of dropout, if used


# training parameters
batch_size: 1024  # batch size for training (on Colab -> 8192)
valid_batch_size: 1   # batch size for validation (simulation)
test_batch_size: 1  # batch size for testing (simulation)

data_loader_workers: 4  # num of processes for data loading
pin_memory: True  # if True, data loader will copy Tensors into CUDA pinned memory before returning them

learning_rate: 0.001  # learning rate of optimizer
weight_decay: 0.0001   # weight decay of optimizer (implemented as L2 penalty)
max_epoch: 200  # maximum number of epochs of training

validate_every: 8   # how many epochs between each validation
log_interval: 1    # how many batches passes between screen updates
max_step_factor: 2  # multiplicative factor for simulation max step (target makespan * max_step_factor)